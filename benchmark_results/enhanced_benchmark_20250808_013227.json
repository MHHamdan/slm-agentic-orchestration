{
  "timestamp": "2025-08-08T01:32:27.777350",
  "analysis": {
    "gpt-4o-mini": {
      "overall": {
        "accuracy": 1.0,
        "avg_latency_ms": 253.42592120170593,
        "total_cost": 0.03,
        "task_count": 200,
        "success_rate": 1.0
      },
      "by_dataset": {
        "HumanEval": {
          "accuracy": 1.0,
          "avg_latency_ms": 253.35827827453613,
          "total_cost": 0.0075,
          "task_count": 50,
          "success_rate": 1.0
        },
        "SQuAD v1.1": {
          "accuracy": 1.0,
          "avg_latency_ms": 252.35735416412354,
          "total_cost": 0.015,
          "task_count": 100,
          "success_rate": 1.0
        },
        "MBPP": {
          "accuracy": 1.0,
          "avg_latency_ms": 255.63069820404053,
          "total_cost": 0.0075,
          "task_count": 50,
          "success_rate": 1.0
        }
      },
      "by_domain": {
        "code_generation": {
          "accuracy": 1.0,
          "avg_latency_ms": 254.49448823928833,
          "total_cost": 0.015,
          "task_count": 100,
          "success_rate": 1.0
        },
        "document_qa": {
          "accuracy": 1.0,
          "avg_latency_ms": 252.35735416412354,
          "total_cost": 0.015,
          "task_count": 100,
          "success_rate": 1.0
        }
      },
      "by_complexity": {
        "simple": {
          "accuracy": 1.0,
          "avg_latency_ms": 255.29931231242855,
          "total_cost": 0.006149999999999999,
          "task_count": 41,
          "success_rate": 1.0
        },
        "moderate": {
          "accuracy": 1.0,
          "avg_latency_ms": 253.6800926878252,
          "total_cost": 0.019649999999999997,
          "task_count": 131,
          "success_rate": 1.0
        },
        "complex": {
          "accuracy": 1.0,
          "avg_latency_ms": 249.49358190808977,
          "total_cost": 0.0042,
          "task_count": 28,
          "success_rate": 1.0
        }
      }
    },
    "phi-3-mini": {
      "overall": {
        "accuracy": 0.98,
        "avg_latency_ms": 61.60868763923645,
        "total_cost": 0.002,
        "task_count": 200,
        "success_rate": 0.98
      },
      "by_dataset": {
        "HumanEval": {
          "accuracy": 0.94,
          "avg_latency_ms": 62.21488952636719,
          "total_cost": 0.0005,
          "task_count": 50,
          "success_rate": 0.94
        },
        "SQuAD v1.1": {
          "accuracy": 0.99,
          "avg_latency_ms": 61.18698596954346,
          "total_cost": 0.001,
          "task_count": 100,
          "success_rate": 0.99
        },
        "MBPP": {
          "accuracy": 1.0,
          "avg_latency_ms": 61.8458890914917,
          "total_cost": 0.0005,
          "task_count": 50,
          "success_rate": 1.0
        }
      },
      "by_domain": {
        "code_generation": {
          "accuracy": 0.97,
          "avg_latency_ms": 62.03038930892944,
          "total_cost": 0.001,
          "task_count": 100,
          "success_rate": 0.97
        },
        "document_qa": {
          "accuracy": 0.99,
          "avg_latency_ms": 61.18698596954346,
          "total_cost": 0.001,
          "task_count": 100,
          "success_rate": 0.99
        }
      },
      "by_complexity": {
        "simple": {
          "accuracy": 0.9512195121951219,
          "avg_latency_ms": 62.39496789327482,
          "total_cost": 0.00041000000000000005,
          "task_count": 41,
          "success_rate": 0.9512195121951219
        },
        "moderate": {
          "accuracy": 0.9923664122137404,
          "avg_latency_ms": 61.20169617747533,
          "total_cost": 0.0013100000000000002,
          "task_count": 131,
          "success_rate": 0.9923664122137404
        },
        "complex": {
          "accuracy": 0.9642857142857143,
          "avg_latency_ms": 62.36148732049124,
          "total_cost": 0.00028000000000000003,
          "task_count": 28,
          "success_rate": 0.9642857142857143
        }
      }
    }
  },
  "baseline_comparisons": [
    {
      "dataset_name": "HumanEval",
      "our_slm_score": 0.94,
      "our_llm_score": 1.0,
      "published_baselines": {
        "GPT-4": 0.67,
        "GPT-3.5": 0.48,
        "CodeT5": 0.2,
        "InCoder-6.7B": 0.15,
        "CodeGen-6B": 0.29
      },
      "metric_name": "pass@1"
    },
    {
      "dataset_name": "SQuAD v1.1",
      "our_slm_score": 0.99,
      "our_llm_score": 1.0,
      "published_baselines": {
        "Human": 0.917,
        "GPT-3": 0.85,
        "BERT-Large": 0.85,
        "RoBERTa-Large": 0.88
      },
      "metric_name": "F1 Score"
    },
    {
      "dataset_name": "MBPP",
      "our_slm_score": 1.0,
      "our_llm_score": 1.0,
      "published_baselines": {
        "GPT-4": 0.52,
        "GPT-3": 0.28,
        "CodeT5": 0.16
      },
      "metric_name": "pass@1"
    }
  ],
  "summary": {
    "datasets_evaluated": 3,
    "community_benchmarks": true,
    "publication_ready": true
  }
}